# -*- coding: utf-8 -*-
"""stats.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18QHEOHbejsr7ZoACEJiJRJCmQl4_sDrd
"""

import numpy as np
import pandas as pd
import scipy.stats as stats

"""# Z-Test

---


"""

# Used when we want to compare the sample mean to the population mean
def zValue(xBar, mu, sigma, n):
    return (xBar - mu) / (sigma / np.sqrt(n))
# Used if we want to detect the critical sample mean
def xBar(z, mu, sigma, n):
    return mu + z * (sigma / np.sqrt(n))

# Used when we compare the mean of two samples
def zTwoSample(xBar1, xBar2, sigma1, sigma2, n1, n2):
    return (xBar1 - xBar2) / np.sqrt(sigma1**2 / n1 + sigma2**2 / n2)

# Used when we compare propotions within a sample
def zProp1(n, success, mean):
  p = success / n
  p0 = mean / n
  return (p - p0) / np.sqrt(p0 * (1 - p0) / n)

# Used when we compare propotions in two samples (number of successes)
def zProp2(n1, success1, n2, success2):
    pHat1 = success1 / n1
    pHat2 = success2 / n2
    pooled = (success1 + success2) / (n1 + n2)
    se = np.sqrt(pooled * (1 - pooled) * (1/n1 + 1/n2))
    return (pHat1 - pHat2) / se

z = zValue(1.85, 1.7, 0.5, 30)
print(z)
print(stats.norm.cdf(z))
z_critical = stats.norm.ppf(0.98)

print(f"Critical Z-score for 96% confidence level: {z_critical}")

"""# T-Test
---


"""

# One sample T-Test
def tValue(xBar, mu, sigma, n):
    return (xBar - mu) / (sigma / np.sqrt(n))

# OR stats.ttest_1sam(data, mean)

# Two sample T-Test
def tTwoSample(xBar1, xBar2, sigma1, sigma2, n1, n2):
    pooled_variance =
    return (xBar1 - xBar2) / np.sqrt(sigma1**2 / n1 + sigma2**2 / n2)
# OR stats.ttest_ind()

# Paired T-Test when we have two related samples: the weights of individuals before and after participating in the Zumba training.

t_statistic, p_value = stats.ttest_1samp(
    [177.3, 182.7, 169.6, 176.3, 180.3, 179.4, 178.5, 177.2, 181.8, 176.5],
    175.3)
print(t_statistic, p_value)

Brand_A = [19.60, 18.82, 19.00, 18.45, 19.79, 19.03, 17.89, 19.42]
Brand_B = [21.10, 20.00, 20.43, 19.67, 18.99, 19.98, 20.14, 19.78]

t_statistic, p_value = stats.ttest_ind(
    b = Brand_A,
    a = Brand_B,)
print(t_statistic, p_value/2)

t_statistic, p_value = stats.ttest_ind(
    b = [13.3, 6.0, 20.0, 8.0, 14.0, 19.0, 18.0, 25.0, 16.0, 24.0, 15.0, 1.0, 15.0],
    a = [22.0, 16.0, 21.7, 21.0, 30.0, 26.0, 12.0, 23.2, 28.0, 23.0],
    equal_var=True,
    alternative='less')
print(t_statistic, p_value)

t_statistic, p_value = stats.ttest_rel(
    [85, 74, 63.5, 69.4, 71.6, 65,90,78],
    [82, 71, 64, 65.2, 67.8, 64.7,95,77],
    alternative='greater')
print(t_statistic, p_value)

z = zValue(3.5, 3, 0.8, 25)
print(z)
print(1-stats.norm.cdf(z))



z = zTwoSample(2.87, 2.56, 1.08, 1.28, 150, 200)
print(z)
print(1 - stats.norm.cdf(z))

z = zProp1(750, 495, 750*0.6)
print(z)
print((1-stats.norm.cdf(z)))

z = zProp2(195, 41, 605, 351)
print(z)

print(2 * stats.norm.cdf(z))

z = zTwoSample(120, 110, 15, 12, 30, 35)
print(z)
print(2 * (1- stats.norm.cdf(z)))

z = zProp1(100, 65, 70)

z

1 - stats.norm.cdf(z)



"""# Chi-squared

### Chi-Squared Test of Independence
**Function**: `scipy.stats.chi2_contingency`

**Usage**: This function is used to test the independence of two categorical variables in a contingency table.
"""

statistic, pvalue, dof, expected_freq = stats.chi2_contingency([
    [335,348, 318],
    [35, 23, 50],
])
print(statistic, pvalue)

"""### Chi-Squared Goodness of Fit Test
**Function**: `scipy.stats.chisquare`

**Usage**: This function is used to test if a sample data fits a specific distribution.
"""

stats.chisquare(f_obs = [45, 50,55], f_exp = [45,60,45])



"""# ANOVA

This test helps determine whether there are statistically significant differences between the means of three or more independent (unrelated) groups.


### Levene's Test and Bartlett's Test:

`f_statistic, p_value = stats.levene([...])`

`f_statistic, p_value = stats.bartlett([...])`

**Purpose**: These tests specifically assess whether the variances of two or more groups are equal. They are designed to test the homogeneity of variances (homoscedasticity).

**Levene's Test**: It is more robust to deviations from normality and is often preferred when the normality assumption may not hold.

**Bartlett's Test**: It assumes normal distributions and is more sensitive to departures from normality. It is best used when the data is approximately normally distributed.

### ANOVA (f_oneway):

`f_statistic, p_value = stats.f_oneway([...])`

**Purpose**: The one-way ANOVA test is used to assess whether there are significant differences between the means of two or more groups. It implicitly assumes that the variances of the groups are equal (homoscedasticity) as one of its assumptions.

**Variance Testing**: ANOVA does not test for equality of variances directly; instead, it tests for differences in means while assuming that variances are equal. If the variances are not equal, the ANOVA results might be unreliable.

## Test Steps:

**a. Independence of Observations**
This is generally ensured by the study design and random sampling. No formal test is usually applied.

**b. Normality**
You can check if each group's data is approximately normally distributed using visual methods (like Q-Q plots) or statistical tests (like the Shapiro-Wilk test).

`_, p_value_A = stats.shapiro()`

**c. Homogeneity of Variances**
You can use Levene's test or Bartlett's test to check if the variances across the groups are equal.

If the assumptions are met, proceed with ANOVA. Otherwise, you should consider alternative approaches or modifications to ensure valid statistical analysis. Something like data transformation or kruskal test

`stats.kruskal([...])`
"""

cars = [19.9, 15.3, 2.2, 6.8, 34.2, 8.3, 12.0, 7.0, 9.5, 1.1]
buses = [1.8, 24.6, 7.2, 37.0, 7.2, 21.2, 6.5, 23.6]
trucks = [13.3, 23.0, 25.4, 15.3, 57.1, 14.5, 26.0]

_, p_value = stats.f_oneway(cars, buses, trucks)
print(p_value)

